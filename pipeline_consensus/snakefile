
# snakemake --profile configs/slurm2 --batch run_main=1/1 
# snakemake --profile configs/slurm2 all
# snakemake --profile configs/slurm all



import os
from datetime import datetime
import time
import re
from shutil import copyfile

import pandas as pd
import re

configfile: "config.json"

title = config["title"]

out_base = config["out_base"]
out_dir = out_base + title

samples_path = config["input_base"] + title + ".tab"
batch_path = config["batch_paths"][title]

reference = config["reference"]
bed_file = config["bed_file"]


df = pd.read_table(samples_path, sep="\t")


# append the paths to the sample names
df["forward"] = batch_path + df["forward"]
df["reverse"] = batch_path + df["reverse"]
# TODO: Consider making an automatical sample-parsing function with regex



print("title:", title)
print("data frame:")
print(df)

print("//")

# _ is a dataframe with one line per sample, so that many sample_libraries can be merged together.
# I think it is only used in rule merge_multiple_libraries

_ = df.groupby("sample").sum() # So you can merge more runs from the same isolate?
#lib_delim = config["library_delimiter"]

print(_)


# Somehow format the sample_library column by some delimiter and join it with underscores.
#_["sample_library"] = _["sample_library"].apply(lambda x: str(x).split(lib_delim)[0] +"_" + "_".join(re.findall("L[0-9]+", x)))




rule all:
    input:
        #expand("{out_dir}/consensus_sequences/{sample}.fa", out_dir = out_dir, sample = _["sample_library"])
        expand(["{out_dir}/consensus_sequences/{sample}.fa",
                "{out_dir}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv",
                "{out_dir}/trimmed_bams/{sample}.trimmed.sorted.amplicon_depth.tsv"],
               out_dir = out_dir, sample = df["sample"])


rule call_consensus:
    input:
        "{out_dir}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        "{out_dir}/consensus_sequences/{sample}.fa"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
        samtools mpileup -A -Q 0 -d 0 {input} | ivar consensus -p {output} -m 10 -n N
        """

#rule variants: # Calls variants from the alignment (TODO)

rule depth:
    input:
        "{out_dir}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        ["{out_dir}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv",
         "{out_dir}/trimmed_bams/{sample}.trimmed.sorted.amplicon_depth.tsv"]
    params:
        bed="{bed}".format(bed = bed_file)
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """

        
        samtools depth -a {input} -o {output[0]}
        samtools depth -b {params.bed} {input} -o {output[1]}

        """


rule trim_reads:
    input:
        #"{out_dir}/merged_aligned_bams/{sample}.sorted.bam"
        "{out_dir}/aligned_bams/{sample}.sorted.bam"
    output:
        "{out_dir}/trimmed_bams/{sample}.trimmed.sorted.bam"
    params:
        bed="{bed}".format(bed = bed_file),
        tmp="{out_dir}/trimmed_bams/{sample}.trimmed.bam"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
        ivar trim -e -i {input} -b {params.bed} -p {params.tmp}
        samtools sort -T {wildcards.sample}.trim -o {output} {params.tmp}
        rm {params.tmp}
        """


rule align_reads:
    input:
        #lambda wildcards: df[df["sample_library"]==wildcards.sample][["forward", "reverse"]].values[0].tolist()
        lambda wildcards: df[df["sample"]==wildcards.sample][["forward", "reverse"]].values[0].tolist()

    output:
        "{out_dir}/aligned_bams/{sample}.sorted.bam"
    params:
        ref= "{ref}".format(ref = reference),
        tmp="{out_dir}/aligned_bams/{sample}.sorted.tmp.bam"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
    

        mkdir -p {out_dir} # Does it automatically create this dir?

        bwa mem {params.ref} {input[0]} {input[1]} | samtools view -F 4 -Sb | samtools sort -T {wildcards.sample}.align -o {params.tmp}


        samtools addreplacerg -r "ID:{wildcards.sample}" -o {output} {params.tmp}
        rm {params.tmp}
        """
