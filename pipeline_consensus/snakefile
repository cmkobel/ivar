
# snakemake --profile configs/slurm2 --batch run_main=1/1 
# snakemake --profile configs/slurm2 all
# snakemake --profile configs/slurm all



print("""      ___     __        
     (_) \\   / /_ _ _ __ 
     | |\\ \\ / / _` | '__|
     | | \\ V / (_| | |   
     |_|  \\_/ \\__,_|_|   
""")


import os
from datetime import datetime
import time
import re
from shutil import copyfile

import pandas as pd
import re

configfile: "config.json"

title = config["title"]

out_base = config["out_base"]
out_dir = out_base + title



samples_path = config["input_base"] + "/" + title + ".tab"
batch_path = config["batch_paths"][title]

reference = config["reference"]
annotation = config["annotation"]
bed_file = config["bed_file"]
bed_insert_file = config["bed_insert_file"]
cowplot_source = config["cowplot_source"]


df = pd.read_table(samples_path, sep="\t")


#df["basename_forward"] = df["forward"]
#df["basename_reverse"] = df["reverse"]

# append the paths to the sample names
# TODO: Consider making an automatical sample-parsing function with regex



print("title:", title)
print("path:", batch_path)
print("data frame:")
print(df)

print("//")


# a little bit of input validation:
sample_name_lengths = set(df["sample"].str.len().values.tolist())
if sample_name_lengths != {9}:
    print("\nNotice: Please make sure that the length of the \"sample\"-column is exactly 9 letters long.\nThe given sample names have the lengths", sample_name_lengths,
          "\nThe mads-format specifies LYYNNNNNN, where L is the sample type letter, YY is the year and NNNNNN is the sample no.",
          "\nExample: I20274139\n//\n\n",
          file = sys.stdout)


df["forward_path"] = batch_path + "/" + df["forward"]
df["reverse_path"] = batch_path + "/" +df["reverse"]

df["batch"] = title

df["full_name"] = title + "_" + df["sample"]


onerror:
    print("An error occurred")
    shell("mail -s 'ivar pipeline error' kobel@pm.me < {log}")



# TODO: move all isolate data into isolates/
# TODO: use double bracket encapsulation for the parameters out_base and title ({{}})


rule all:
    input:
        #expand("{out_dir}/consensus_sequences/{sample}.fa", out_dir = out_dir, sample = _["sample_library"])
        expand(["{out_base}/{sample}/consensus_sequences/{sample}.fa",
                "{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv",
                "{out_base}/{sample}/aligned_bams/{sample}.sorted.bam",
                "{out_base}/{sample}/trimmed_bams/{sample}_snps.tsv",
                "{out_base}/{sample}/pangolin/{sample}_pangolin.csv",
                "{out_base}/{sample}/nextclade/{sample}_nextclade.csv",
                "{out_base}/{title}_coverage.png",
                "{out_base}/all_nextclade.csv",
                "{out_base}/all_pangolin.csv",
                "{out_base}/{title}_input_list.tab",
                "{out_base}/all_input_lists.tab",
                "{out_base}/{sample}/consensus_sequences/{sample}_alt.fa"],
               sample = df["full_name"],
               out_base = out_base,
               title = title)
    


# TODO: this rule might become deprecated as scripts/collect.sh matures
rule collect:
    input:
        expand(["{out_base}/{sample}/pangolin/{sample}_pangolin.csv",
                "{out_base}/{sample}/nextclade/{sample}_nextclade.csv"],
                sample = df["full_name"], 
                out_base = out_base)
    output:
        ["{out_base}/all_pangolin.csv",
         "{out_base}/all_nextclade.csv",
         "{out_base}/all_input_lists.tab"]
    params: 
        names = "\t".join(df.columns.tolist()) # assuming that all files have the same headers in the same order

    shell:
        """



        ./scripts/collect.sh



        """



rule batch: # Runs once per batch. Outputs the pandas dataframe that snakemake runs with. 
    input:
        expand("{out_base}/{sample}/consensus_sequences/{sample}.fa",
            out_dir = out_dir,
               sample = df["full_name"], 
               out_base = out_base)
    output:
        "{out_base}/{title}_input_list.tab"
    params:
        csv_out = df.to_csv(sep='\t', encoding='utf-8', index = False)
    shell:
        """
        # collect all input lists
        echo '''{params.csv_out}''' > {output}

        """






rule nextclade:
    input:
        "{out_base}/{sample}/consensus_sequences/{sample}.fa"
    output:
        "{out_base}/{sample}/nextclade/{sample}_nextclade.csv"
    params:
        bind_base = "{out_base}/{sample}",
        rel_in = "consensus_sequences/{sample}.fa",
        rel_out = "nextclade/{sample}_nextclade.csv"
    shell:
        """
        singularity run --bind {params.bind_base}:/sample_dir \
            docker://neherlab/nextclade:0.7.5 \
                nextclade.js \
                    --input-fasta /sample_dir/{params.rel_in} \
                    --output-csv /sample_dir/{params.rel_out}.tmp

        cat {output}.tmp \
        | awk -v sam={wildcards.sample} '{{ print sam ";" $0 }}' \
        > {output}

        rm {output}.tmp
        
        """



rule pangolin:
    input:
        "{out_base}/{sample}/consensus_sequences/{sample}.fa"
    output:
        [directory("{out_base}/{sample}/pangolin/"),
         "{out_base}/{sample}/pangolin/{sample}_pangolin.csv"]
    conda:
        "envs/pangolin.yml" # This might be a bad idea (slow)
    threads:
        1
    shell:
        """
        
        touch {output[1]}

        pangolin {input} \
                --threads 1 \
                --outdir {output[0]}

        cat {output[0]}/lineage_report.csv | awk -v sam={wildcards.sample} '{{ print sam "," $0 }}' > {output[1]}
        rm {output[0]}/lineage_report.csv
        #mv {output[0]}/lineage_report.csv {output[1]}

        """


rule call_consensus:
    input:
        "{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        "{out_base}/{sample}/consensus_sequences/{sample}.fa"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
        samtools mpileup -A -Q 0 -d 0 {input} | ivar consensus -q 30 -p {output} -m 5 -n N

        """


# http://www.metagenomics.wiki/tools/samtools/consensus-sequence
# I'm trying to make a consensus genome which has the same length of the reference.
# This way it is easier to compare the sites between our samples and the reference.
rule call_consesus_alt:
    input:
        "{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        ["{out_base}/{sample}/consensus_sequences/{sample}_alt.fa"]

    conda:
        "envs/alt_consensus.yml"
    params: 
        base = "{out_base}/{sample}/consensus_sequences",
        ref = "{ref}".format(ref = reference)
    resources:
        mem_mb = 8000,
        time = "01:00:00"
    shell:
        """
        
        echo "samtools ..."
        # Get consensus fastq file
        samtools mpileup -uf {params.ref} {input} | bcftools call -c | vcfutils.pl vcf2fq > {params.base}/SAMPLE_cns.fastq


        # vcfutils.pl is part of bcftools


        echo "seqtk ..."
        # Convert .fastq to .fasta and set bases of quality lower than 30 to N
        seqtk seq -aQ64 -q30 -n N {params.base}/SAMPLE_cns.fastq > {params.base}/SAMPLE_cns.fasta

        mv {params.base}/SAMPLE_cns.fasta {output}

        """




rule call_consensus_q20:
    input:
        "{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        "{out_base}/{sample}/consensus_sequences/{sample}_q20.fa"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
        samtools mpileup -A -Q 0 -d 0 {input} | ivar consensus -q 20 -p {output} -m 10 -n N

        """





rule variants: # Calls variants from the alignment (TODO)
    input:
        "{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        "{out_base}/{sample}/trimmed_bams/{sample}_snps.tsv"
    params:
        tmp_noex = "{out_base}/{sample}/trimmed_bams/{sample}_snps_tmp"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """

        samtools mpileup -aa -A -d 600000 -B -Q 0 {input} \
        | ivar variants -p {params.tmp_noex} -q 20 -t 0.03 -r {reference} -g {annotation}

        cat {params.tmp_noex}.tsv | awk -v sam={wildcards.sample} '{{ print sam "\\t" $0 }}' > {output}
        rm {params.tmp_noex}.tsv
        
        """

rule depth_fig:
    input:
        expand(["{out_base}/{title}_coverage.tab",
                "{out_base}/all_nextclade.csv"],
                out_base = out_base,
                title = title)
    output:
         "{out_base}/{title}_coverage.png"
    shell:
        """

        
        1>&2 echo "singularitieing ..."
        singularity run docker://rocker/tidyverse \
            Rscript scripts/coverage_figure.r \
                {input[0]} \
                {input[1]} \
                {bed_file} \
                {bed_insert_file} \
                {output} 
                #{cowplot_source}
        
        """




rule collect_depth:
    input:
        expand(["{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv", 
                "{out_base}/{sample}/aligned_bams/{sample}.sorted.depth.tsv"],
            out_base = out_base,
            title = title,
            sample = df["full_name"])
    output:
        "{out_base}/{title}_coverage.tab"
    shell:
        """
        
        1>&2 echo "catting ..."
        cat {input} > {output}
        
        """



rule depth:
    input:
        ["{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.bam",
         "{out_base}/{sample}/aligned_bams/{sample}.sorted.bam"]
    output:
        ["{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv",
         "{out_base}/{sample}/aligned_bams/{sample}.sorted.depth.tsv"]#,
         #"{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.amplicon_depth.tsv"]
    params:
        bed="{bed}".format(bed = bed_file)
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """

        samtools depth -m 0 -a {input[0]} | awk -v sam={wildcards.sample} '{{ print sam "\\tafter trimming\\t" $0 }}' > {output[0]}
        samtools depth -m 0 -a {input[1]} | awk -v sam={wildcards.sample} '{{ print sam "\\tbefore trimming\\t" $0 }}' > {output[1]}


        """


rule trim_reads:
    input:
        #"{out_dir}/merged_aligned_bams/{sample}.sorted.bam"
        #"{out_dir}/aligned_bams/{sample}.sorted.bam"
        "{out_base}/{sample}/aligned_bams/{sample}.sorted.bam"

    output:
        #"{out_dir}/trimmed_bams/{sample}.trimmed.sorted.bam"
        "{out_base}/{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"

    params:
        bed ="{bed}".format(bed = bed_file),
        tmp ="{out_base}/{sample}/trimmed_bams/{sample}.trimmed.bam"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
        ivar trim -e -i {input} -b {params.bed} -p {params.tmp} -q 30
        samtools sort -T {wildcards.sample}.trim -o {output} {params.tmp}
        rm {params.tmp}
        """

# TODO: Consider spliting adapter-trimming and alignment up into two rules
rule align_reads:
    input:
        #lambda wildcards: df[df["sample_library"]==wildcards.sample][["forward_path", "reverse_path"]].values[0].tolist()
        lambda wildcards: df[df["full_name"]==wildcards.sample][["forward_path", "reverse_path"]].values[0].tolist()

    output:
        #"{out_dir}/aligned_bams/{sample}.sorted.bam"
        "{out_base}/{sample}/aligned_bams/{sample}.sorted.bam"
    params:
        ref = "{ref}".format(ref = reference),
        trim_path = "{out_base}/{sample}/adapters_trimmed/",
        tmp ="{out_base}/{sample}/aligned_bams/{sample}.sorted.tmp.bam"
    conda:
        "envs/ivar-inpipe.yml"
    threads:
        4
    shell:
        """
        
        trim_galore --paired --fastqc --cores 4 --gzip -o {params.trim_path} --basename {wildcards.sample} {input[0]} {input[1]}  

        
        bwa mem -t 4 {reference} {params.trim_path}/{wildcards.sample}_val_1.fq.gz {params.trim_path}/{wildcards.sample}_val_2.fq.gz \
        | samtools view -F 4 -Sb -@ 4 \
        | samtools sort -@ 4 -T {wildcards.sample}.align -o {params.tmp}


        samtools addreplacerg -@ 4 -r "ID:{wildcards.sample}" -o {output} {params.tmp}
        rm {params.tmp}

        """



