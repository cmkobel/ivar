
# snakemake --profile configs/slurm2 --batch run_main=1/1 
# snakemake --profile configs/slurm2 all
# snakemake --profile configs/slurm all



import os
from datetime import datetime
import time
import re
from shutil import copyfile

import pandas as pd
import re

configfile: "config.json"

title = config["title"]

out_base = config["out_base"]
out_dir = out_base + title



samples_path = config["input_base"] + "/" + title + ".tab"
batch_path = config["batch_paths"][title]

reference = config["reference"]
annotation = config["annotation"]
bed_file = config["bed_file"]
bed_insert_file = config["bed_insert_file"]
cowplot_source = config["cowplot_source"]


df = pd.read_table(samples_path, sep="\t")


#df["basename_forward"] = df["forward"]
#df["basename_reverse"] = df["reverse"]

# append the paths to the sample names
df["forward_path"] = batch_path + "/" + df["forward"]
df["reverse_path"] = batch_path + "/" +df["reverse"]
# TODO: Consider making an automatical sample-parsing function with regex



print("title:", title)
print("data frame:")
print(df)

print("//")





rule all:
    input:
        #expand("{out_dir}/consensus_sequences/{sample}.fa", out_dir = out_dir, sample = _["sample_library"])
        expand(["{out_base}/{title}_{sample}/consensus_sequences/{sample}.fa",
                "{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv",
                #"{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.amplicon_depth.tsv",
                "{out_base}/{title}_{sample}/trimmed_bams/{sample}_snps.tsv",
                "{out_base}/{title}_{sample}/pangolin/{sample}_pangolin.csv",
                "{out_base}/{title}_{sample}/nextclade/{sample}_nextclade.csv",
                "{out_base}/{title}_coverage.png"],
               out_dir = out_dir,
               sample = df["sample"],
               out_base = out_base,
               title = title)
    output:
        ["{out_base}/{title}_nextclade.csv"]

    shell:
        """

        cat {input[4]} > {output[0]}
        

        """



onerror:
    print("An error occurred")
    print("{log}")
    shell("mail -s 'ivar pipeline error' kobel@pm.me < {log}")




rule nextclade:
    input:
        "{out_base}/{title}_{sample}/consensus_sequences/{sample}.fa"
    output:
        "{out_base}/{title}_{sample}/nextclade/{sample}_nextclade.csv"
    params:
        bind_base = "{out_base}/{title}_{sample}",
        rel_in = "consensus_sequences/{sample}.fa",
        rel_out = "nextclade/{sample}_nextclade.csv"
    shell:
        """
        singularity run --bind {params.bind_base}:/sample_dir \
            docker://neherlab/nextclade:0.7.5 \
                nextclade.js \
                    --input-fasta /sample_dir/{params.rel_in} \
                    --output-csv /sample_dir/{params.rel_out}
        """



rule pangolin:
    input:
        "{out_base}/{title}_{sample}/consensus_sequences/{sample}.fa"
    output:
        [directory("{out_base}/{title}_{sample}/pangolin/"),
         "{out_base}/{title}_{sample}/pangolin/{sample}_pangolin.csv"]
    conda:
        "envs/pangolin.yml" # This might be a bad idea (slow)
    threads:
        1
    shell:
        """

        pangolin {input} \
                --threads 1 \
                --outdir {output[0]}
        mv {output[0]}/lineage_report.csv {output[1]}

        """


rule call_consensus:
    input:
        "{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        "{out_base}/{title}_{sample}/consensus_sequences/{sample}.fa"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
        samtools mpileup -A -Q 0 -d 0 {input} | ivar consensus -p {output} -m 10 -n N
        """

rule variants: # Calls variants from the alignment (TODO)
    input:
        "{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        "{out_base}/{title}_{sample}/trimmed_bams/{sample}_snps.tsv"
    params:
        tmp_noex = "{out_base}/{title}_{sample}/trimmed_bams/{sample}_snps_tmp"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """

        samtools mpileup -aa -A -d 600000 -B -Q 0 {input} \
        | ivar variants -p {params.tmp_noex} -q 20 -t 0.03 -r {reference} -g {annotation}

        cat {params.tmp_noex}.tsv | awk -v sam={wildcards.sample} '{{ print sam "\\t" $0 }}' > {output}
        rm {params.tmp_noex}.tsv
        
        """

rule depth_fig:
    input:
        expand(["{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv", 
                "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.depth.tsv"],
            out_base = out_base,
            title = title,
            sample = df["sample"])
    output:
        ["{out_base}/{title}_coverage.tab",
         "{out_base}/{title}_coverage.png"]
    shell:
        """


        
        1>&2 echo "catting ..."
        touch {output[0]}
        cat {input} > {output[0]}
        
        1>&2 echo "singularitieing ..."
        singularity run docker://rocker/tidyverse \
            Rscript scripts/coverage_figure.r \
                {output[0]} \
                {bed_file} \
                {bed_insert_file} \
                {output[1]} #
                #{cowplot_source}
        
        """


rule depth:
    input:
        ["{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.bam",
         "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.bam"]
    output:
        ["{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv",
         "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.depth.tsv"]#,
         #"{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.amplicon_depth.tsv"]
    params:
        bed="{bed}".format(bed = bed_file)
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """

        samtools depth -a {input[0]} | awk -v sam={wildcards.sample} '{{ print sam "\\tafter trimming\\t" $0 }}' > {output[0]}
        samtools depth -a {input[1]} | awk -v sam={wildcards.sample} '{{ print sam "\\tbefore trimming\\t" $0 }}' > {output[1]}


        """


rule trim_reads:
    input:
        #"{out_dir}/merged_aligned_bams/{sample}.sorted.bam"
        #"{out_dir}/aligned_bams/{sample}.sorted.bam"
        "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.bam"

    output:
        #"{out_dir}/trimmed_bams/{sample}.trimmed.sorted.bam"
        "{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"

    params:
        bed ="{bed}".format(bed = bed_file),
        tmp ="{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.bam"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
        ivar trim -e -i {input} -b {params.bed} -p {params.tmp}
        samtools sort -T {wildcards.sample}.trim -o {output} {params.tmp}
        rm {params.tmp}
        """

# TODO: Consider spliting adapter-trimming and alignment up into two rules
rule align_reads:
    input:
        #lambda wildcards: df[df["sample_library"]==wildcards.sample][["forward_path", "reverse_path"]].values[0].tolist()
        lambda wildcards: df[df["sample"]==wildcards.sample][["forward_path", "reverse_path"]].values[0].tolist()

    output:
        #"{out_dir}/aligned_bams/{sample}.sorted.bam"
        "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.bam"
    params:
        ref = "{ref}".format(ref = reference),
        trim_path = "{out_base}/{title}_{sample}/adapters_trimmed/",
        tmp ="{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.tmp.bam"
    conda:
        "envs/ivar-inpipe.yml"
    threads:
        4
    shell:
        """
        
        trim_galore --paired --fastqc --cores 4 --gzip -o {params.trim_path} --basename {wildcards.sample} {input[0]} {input[1]}  

        
        bwa mem -t 4 {reference} {params.trim_path}/{wildcards.sample}_val_1.fq.gz {params.trim_path}/{wildcards.sample}_val_2.fq.gz \
        | samtools view -F 4 -Sb -@ 4 \
        | samtools sort -@ 4 -T {wildcards.sample}.align -o {params.tmp}


        samtools addreplacerg -@ 4 -r "ID:{wildcards.sample}" -o {output} {params.tmp}
        rm {params.tmp}

        """
