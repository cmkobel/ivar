
# snakemake --profile configs/slurm2 --batch run_main=1/1 
# snakemake --profile configs/slurm2 all
# snakemake --profile configs/slurm all



import os
from datetime import datetime
import time
import re
from shutil import copyfile

import pandas as pd
import re

configfile: "config.json"

title = config["title"]

out_base = config["out_base"]
out_dir = out_base + title



samples_path = config["input_base"] + "/" + title + ".tab"
batch_path = config["batch_paths"][title]

reference = config["reference"]
annotation = config["annotation"]
bed_file = config["bed_file"]
bed_insert_file = config["bed_insert_file"]
cowplot_source = config["cowplot_source"]


df = pd.read_table(samples_path, sep="\t")


#df["basename_forward"] = df["forward"]
#df["basename_reverse"] = df["reverse"]

# append the paths to the sample names
# TODO: Consider making an automatical sample-parsing function with regex



print("title:", title)
print("path:", batch_path)
print("data frame:")
print(df)

print("//")

df["forward_path"] = batch_path + "/" + df["forward"]
df["reverse_path"] = batch_path + "/" +df["reverse"]

df["batch"] = title


onerror:
    print("An error occurred")
    shell("mail -s 'ivar pipeline error' kobel@pm.me < {log}")



# TODO: move all isolate data into isolates/
# TODO: use double bracket encapsulation for the parameters out_base and title ({{}})


rule all:
    input:
        #expand("{out_dir}/consensus_sequences/{sample}.fa", out_dir = out_dir, sample = _["sample_library"])
        expand(["{out_base}/{title}_{sample}/consensus_sequences/{sample}.fa",
                "{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv",
                "{out_base}/{title}_{sample}/trimmed_bams/{sample}_snps.tsv",
                "{out_base}/{title}_{sample}/pangolin/{sample}_pangolin.csv",
                "{out_base}/{title}_{sample}/nextclade/{sample}_nextclade.csv",
                "{out_base}/{title}_coverage.png",
                "{out_base}/all_nextclade.csv",
                "{out_base}/all_pangolin.csv",
                "{out_base}/{title}_input_list.tab",
                "{out_base}/all_input_lists.tab"],
               out_dir = out_dir,
               sample = df["sample"],
               out_base = out_base,
               title = title)
    


rule collect:
    input:
        expand(["{out_base}/{title}_{sample}/pangolin/{sample}_pangolin.csv",
                "{out_base}/{title}_{sample}/nextclade/{sample}_nextclade.csv"],
                sample = df["sample"], 
                title = title,
                out_base = out_base)
    output:
        ["{out_base}/all_pangolin.csv",
         "{out_base}/all_nextclade.csv",
         "{out_base}/all_input_lists.tab"]
    params: 
        names = "\t".join(df.columns.tolist()) # assuming that all files have the same headers in the same order

    shell:
        """

        # collect pangolin 
        echo "sample,taxon,lineage,SH-alrt,UFbootstrap,lineages_version,status,note" \
        > {output[0]}
        cat {out_base}/*/pangolin/*pangolin.csv | grep -v "taxon,lineage," >> {output[0]}
        

        # collect nextclade
        echo "sample,seqName;substitutions;totalMutations;aminoacidChanges;totalAminoacidChanges;insertions;totalInsertions;deletions;totalGaps;missing;totalMissing;nonACGTNs;totalNonACGTNs;alignmentStart;alignmentEnd;alignmentScore;pcrPrimerChanges;totalPcrPrimerChanges;clade;qc.seqName;qc.privateMutations.score;qc.privateMutations.total;qc.privateMutations.excess;qc.privateMutations.cutoff;qc.privateMutations.status;qc.missingData.score;qc.missingData.totalMissing;qc.missingData.missingDataThreshold;qc.missingData.status;qc.snpClusters.score;qc.snpClusters.totalSNPs;qc.snpClusters.clusteredSNPs;qc.snpClusters.status;qc.mixedSites.score;qc.mixedSites.totalMixedSites;qc.mixedSites.mixedSitesThreshold;qc.mixedSites.status;qc.overallScore;qc.overallStatus" \
        > {output[1]}
        cat {out_base}/*/nextclade/*_nextclade.csv | grep -v "seqName;substitutions;" >> {output[1]}

        
        # collect input list files
        echo -e "forward\\treverse\\tsample\\tcomment\\tforward_path\\treverse_path\\tbatch" 
        echo -e "{params.names}" > {output[2]}
        cat ../output/*input_list.tab | grep -vP "forward\\treverse" >> {output[2]}



        """



rule batch: # Runs once per batch. Outputs the pandas dataframe that snakemake runs with. 
    input:
        expand("{out_base}/{title}_{sample}/consensus_sequences/{sample}.fa",
            out_dir = out_dir,
               sample = df["sample"],
               out_base = out_base,
               title = title)
    output:
        "{out_base}/{title}_input_list.tab"
    params:
        csv_out = df.to_csv(sep='\t', encoding='utf-8', index = False)
    shell:
        """
        # collect all input lists
        echo '''{params.csv_out}''' > {output}

        """






rule nextclade:
    input:
        "{out_base}/{title}_{sample}/consensus_sequences/{sample}.fa"
    output:
        "{out_base}/{title}_{sample}/nextclade/{sample}_nextclade.csv"
    params:
        bind_base = "{out_base}/{title}_{sample}",
        rel_in = "consensus_sequences/{sample}.fa",
        rel_out = "nextclade/{sample}_nextclade.csv"
    shell:
        """
        singularity run --bind {params.bind_base}:/sample_dir \
            docker://neherlab/nextclade:0.7.5 \
                nextclade.js \
                    --input-fasta /sample_dir/{params.rel_in} \
                    --output-csv /sample_dir/{params.rel_out}.tmp

        cat {output}.tmp \
        | awk -v sam={wildcards.sample} '{{ print sam ";" $0 }}' \
        > {output}

        rm {output}.tmp
        
        """



rule pangolin:
    input:
        "{out_base}/{title}_{sample}/consensus_sequences/{sample}.fa"
    output:
        [directory("{out_base}/{title}_{sample}/pangolin/"),
         "{out_base}/{title}_{sample}/pangolin/{sample}_pangolin.csv"]
    conda:
        "envs/pangolin.yml" # This might be a bad idea (slow)
    threads:
        1
    shell:
        """
        
        touch {output[1]}

        pangolin {input} \
                --threads 1 \
                --outdir {output[0]}

        cat {output[0]}/lineage_report.csv | awk -v sam={wildcards.sample} '{{ print sam "," $0 }}' > {output[1]}
        rm {output[0]}/lineage_report.csv
        #mv {output[0]}/lineage_report.csv {output[1]}

        """


rule call_consensus:
    input:
        "{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        "{out_base}/{title}_{sample}/consensus_sequences/{sample}.fa"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
        samtools mpileup -A -Q 0 -d 0 {input} | ivar consensus -p {output} -m 10 -n N
        """

rule variants: # Calls variants from the alignment (TODO)
    input:
        "{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"
    output:
        "{out_base}/{title}_{sample}/trimmed_bams/{sample}_snps.tsv"
    params:
        tmp_noex = "{out_base}/{title}_{sample}/trimmed_bams/{sample}_snps_tmp"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """

        samtools mpileup -aa -A -d 600000 -B -Q 0 {input} \
        | ivar variants -p {params.tmp_noex} -q 20 -t 0.03 -r {reference} -g {annotation}

        cat {params.tmp_noex}.tsv | awk -v sam={wildcards.sample} '{{ print sam "\\t" $0 }}' > {output}
        rm {params.tmp_noex}.tsv
        
        """

rule depth_fig:
    input:
        expand(["{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv", 
                "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.depth.tsv"],
            out_base = out_base,
            title = title,
            sample = df["sample"])
    output:
        ["{out_base}/{title}_coverage.tab",
         "{out_base}/{title}_coverage.png"]
    shell:
        """


        
        1>&2 echo "catting ..."
        touch {output[0]}
        cat {input} > {output[0]}
        
        1>&2 echo "singularitieing ..."
        singularity run docker://rocker/tidyverse \
            Rscript scripts/coverage_figure.r \
                {output[0]} \
                {bed_file} \
                {bed_insert_file} \
                {output[1]} #
                #{cowplot_source}
        
        """


rule depth:
    input:
        ["{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.bam",
         "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.bam"]
    output:
        ["{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.depth.tsv",
         "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.depth.tsv"]#,
         #"{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.amplicon_depth.tsv"]
    params:
        bed="{bed}".format(bed = bed_file)
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """

        samtools depth -a {input[0]} | awk -v sam={wildcards.sample} '{{ print sam "\\tafter trimming\\t" $0 }}' > {output[0]}
        samtools depth -a {input[1]} | awk -v sam={wildcards.sample} '{{ print sam "\\tbefore trimming\\t" $0 }}' > {output[1]}


        """


rule trim_reads:
    input:
        #"{out_dir}/merged_aligned_bams/{sample}.sorted.bam"
        #"{out_dir}/aligned_bams/{sample}.sorted.bam"
        "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.bam"

    output:
        #"{out_dir}/trimmed_bams/{sample}.trimmed.sorted.bam"
        "{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.sorted.bam"

    params:
        bed ="{bed}".format(bed = bed_file),
        tmp ="{out_base}/{title}_{sample}/trimmed_bams/{sample}.trimmed.bam"
    conda:
        "envs/ivar-inpipe.yml"
    shell:
        """
        ivar trim -e -i {input} -b {params.bed} -p {params.tmp}
        samtools sort -T {wildcards.sample}.trim -o {output} {params.tmp}
        rm {params.tmp}
        """

# TODO: Consider spliting adapter-trimming and alignment up into two rules
rule align_reads:
    input:
        #lambda wildcards: df[df["sample_library"]==wildcards.sample][["forward_path", "reverse_path"]].values[0].tolist()
        lambda wildcards: df[df["sample"]==wildcards.sample][["forward_path", "reverse_path"]].values[0].tolist()

    output:
        #"{out_dir}/aligned_bams/{sample}.sorted.bam"
        "{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.bam"
    params:
        ref = "{ref}".format(ref = reference),
        trim_path = "{out_base}/{title}_{sample}/adapters_trimmed/",
        tmp ="{out_base}/{title}_{sample}/aligned_bams/{sample}.sorted.tmp.bam"
    conda:
        "envs/ivar-inpipe.yml"
    threads:
        4
    shell:
        """
        
        trim_galore --paired --fastqc --cores 4 --gzip -o {params.trim_path} --basename {wildcards.sample} {input[0]} {input[1]}  

        
        bwa mem -t 4 {reference} {params.trim_path}/{wildcards.sample}_val_1.fq.gz {params.trim_path}/{wildcards.sample}_val_2.fq.gz \
        | samtools view -F 4 -Sb -@ 4 \
        | samtools sort -@ 4 -T {wildcards.sample}.align -o {params.tmp}


        samtools addreplacerg -@ 4 -r "ID:{wildcards.sample}" -o {output} {params.tmp}
        rm {params.tmp}

        """
